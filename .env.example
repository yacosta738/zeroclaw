# ZeroClaw Environment Configuration
# Copy this file to .env and customize for your setup

# ── LLM Provider Configuration ──

# For cloud providers (OpenRouter, OpenAI, Anthropic): your API key
# For Ollama: the base URL to your Ollama instance
# Examples:
#   Cloud: API_KEY=sk-or-v1-...
#   Ollama Mac/Windows: API_KEY=http://host.docker.internal:11434
#   Ollama Linux: API_KEY=http://172.17.0.1:11434 (or your host IP)
API_KEY=sk-or-v1-your-openrouter-key-here

# LLM Provider
# Options: openrouter, openai, anthropic, ollama
PROVIDER=openrouter

# Model to use (provider-specific)
# Provider → Model examples:
#   openrouter → anthropic/claude-sonnet-4-20250514
#   openai → gpt-4o
#   anthropic → claude-3-sonnet-20240229
#   ollama → llama3.2:latest, mistral, codellama
ZEROCLAW_MODEL=anthropic/claude-sonnet-4-20250514

# ── Gateway Configuration ──

# Port to expose on your host machine
ZEROCLAW_GATEWAY_PORT=3000

# ── Example Configurations ──

# Example 1: OpenRouter (default, cloud)
# API_KEY=sk-or-v1-xxxxxxxx
# PROVIDER=openrouter
# ZEROCLAW_MODEL=anthropic/claude-sonnet-4-20250514

# Example 2: Ollama on Mac/Windows (local)
# API_KEY=http://host.docker.internal:11434
# PROVIDER=ollama
# ZEROCLAW_MODEL=llama3.2:latest

# Example 3: Ollama on Linux (local)
# API_KEY=http://192.168.1.100:11434  # Your host IP
# PROVIDER=ollama
# ZEROCLAW_MODEL=llama3.2:latest

# Example 4: OpenAI (cloud)
# API_KEY=sk-xxxxxxxx
# PROVIDER=openai
# ZEROCLAW_MODEL=gpt-4o
